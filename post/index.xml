<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Lilith Sangreal</title>
    <link>https://lilithsangreal.com/post/</link>
    <description>Recent content in Posts on Lilith Sangreal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Aug 2022 19:46:00 +0800</lastBuildDate><atom:link href="https://lilithsangreal.com/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>iOS 性能指标工具集笔记</title>
      <link>https://lilithsangreal.com/p/ios-%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87%E5%B7%A5%E5%85%B7%E9%9B%86%E7%AC%94%E8%AE%B0/</link>
      <pubDate>Fri, 26 Aug 2022 19:46:00 +0800</pubDate>
      
      <guid>https://lilithsangreal.com/p/ios-%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87%E5%B7%A5%E5%85%B7%E9%9B%86%E7%AC%94%E8%AE%B0/</guid>
      <description>iOS 性能指标工具集笔记 综述 优秀的产品对于应用的功耗表现也有所要求。因此 iOS 在开发过程中会使用 Instruments、Energy Gauges 和 Profiling 来分析和改进应用的功耗表现。对于 Beta 测试，也可以利用 Instruments 进行分析。
不考虑第三方框架，已经发布的 TestFlight / App Store 包，虽然我们仍有机会获取到一些崩溃日志（隐私日志里面的Crash日志等）。但这些很难让开发者量化分析应用的功耗表现，显然，我们需要更多数据。
为此苹果在 2019 年伴随iOS13和 Xcode11 发布了一系列工具用于收集和分析用于手机功耗表现相关的数据，这些工具大量出现了 Metrics 这个关键词，我认为可以直接翻译为指标，其实苹果是给用户提供了更多可量化的指标数据及其相关工具、框架，这是第一批发布的指标工具：
 XCTest Metrics：可以测量部分功能的性能和电池的寿命指标 MetricKit：一个用于收集诸多用户电池和性能表现相关数据的框架 Xcode Metrics Organizer：一个统计大盘数据概率，可以看所有用户的性能表现   上图为苹果描述的应用场景。
 首批提供的指标如下：
 Battery Metrics：提供CPU、位置、显示、网络、蓝牙、多媒体和相册的功耗表现 Performance Metrics：提供挂起、磁盘、应用启动、内存和自定义间隔  在 2020 年，苹果再一次提升了这些指标工具的能力，发布了 Metric Kit2.0，拓展了三个指标集合 5:24
 CPU 指令：能够给出一天内究竟有多少指令执行了，极度精确地给出App究竟做了多少事 滚动故障：用户感受到的动画丢帧/不流畅占正常动画的比例，并进一步介绍如何用XCTest来排查与解决动画故障 APP 退出：给出一天内，APP究竟为什么退出了，退出了多少次。一个应用，除了正常退出，也会有很多情况被苹果干掉  除了指标集合，苹果还推出了可以帮助定位和修复问题的数据工具 MetricKit 2.0 Diagnostics 8:14，包含指标类型如下：
 挂起：采样App对用户长时间无反应（主线程阻塞等）时的堆栈 CPU 异常：采样高CPU占用时的堆栈 磁盘写入异常：采样总写入次数过多，异常过量写入发生时的堆栈（突破每日1GB） Crash：采样崩溃发生时的虚拟内存信息和堆栈  同时，升级了Xcode Metrics Organizer 分析性能指标的能力。</description>
    </item>
    
    <item>
      <title>Protocol Buffers 编译器源码阅读</title>
      <link>https://lilithsangreal.com/p/google-protocol-buffers-notes/</link>
      <pubDate>Fri, 29 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://lilithsangreal.com/p/google-protocol-buffers-notes/</guid>
      <description>Protocol Buffers 编译器源码阅读 前言 Protocol Buffers （简称 pb/protobuf） 是谷歌公司开发的一款跨平台跨语言的结构数据序列化框架，可以使用 protobuf 的提供的编译器将定义好的数据模型（ proto文件 ）编译为各种语言，提供对应的特殊数据结构的序列化、反序列化能力。protobuf 性能好，语法简洁，广泛应用于 rpc 框架中，为其提供编解码的能力。
除此之外，protobuf 还是一个优秀的 C++ 开源项目 Github，在社区里询问阅读什么项目提升自己的 C++ 编码能力时，也有很多人推荐阅读 protobuf 的源码。
出于工作上的原因，最近我也对 protobuf 的编译器源码（Protocol buffers 3）进行一定阅读，发现它的代码整洁，逻辑清晰，设计也非常好。因此写一篇这样的文章记录一下阅读的一些收获。
编译器相关知识 如果完全没有编译器相关的知识，那么阅读 protobuf 的源码是无从下手的。和寻常的编译器一样，protobuf 也可以分为编译器前端和后端，并完成了一个从源语言到目标语言的转换。对于传统编程语言来说，即是从源代码到机器直接执行的机器码的转换。
如图所示，对于 protobuf 来说，源语言就是一个 proto 文件，定义了数据结构。而目标语言则是对应语言的源代码。以便于各种语言直接调用使用。
目前 protobuf 支持的语言如下。也就是说，定义好的 proto 文件会转换为下列语言的源代码。
对于编译器来说，前端主要完成的工作是词法分析，语法分析，语义分析。
假设我们已经完成了预编译器的工作（一般包括去除空格，插入头文件，展开宏等等……事实上，根据不同的语言复杂程度，预编译器的工作也可能会非常复杂）。假设我们的语言非常简单，不需要引入什么头文件，展开宏等等，那么输入的目标语言将会被去除多余的空格和换行符。
这样，对于编译器来说，目标语言只是一串连续的字母和特殊符号的序列，比如：
1  whiletrue;   单个字母和符号一般不能表达什么含义，因此 词法分析 是将单词组合拆分成 token（单词）的形式， 如上述代码中的 while 和 true。经过词法分析，字母和特殊符号的序列就转化为了一串由单词形式组成的序列。
而 语法分析 则是将这个单词的序列组织成一个抽象语法树（Abstract Syntax Tree，AST），后文会针对protobuf使用的语法分析方式介绍这一过程的实例。
至此为止，其实编译器也只是将源语言，从一种不包含结构的序列形式转换成了含有结构的形式。这其中产生了任何的理解了吗？没有，至此位置，计算机只是对数据进行了一些处理工作，但并不知道我们的代码做了什么事情。
但是组织一棵树的形式，则为语言初步理解源代码做了什么，做的事情是否合法提供了条件。编译器会反复阅读这棵树，根据这棵树的结构来分析语义，来判断程序员输入的东西是否合理。在这个过程中，编译器会为AST的每个节点填写一些属性。这就是 语义分析。
LL(1) 文法 </description>
    </item>
    
    <item>
      <title>从 0 开始学习 Transformer 拾遗：文章本身的与解释</title>
      <link>https://lilithsangreal.com/p/transformer-from-zero-4/</link>
      <pubDate>Fri, 17 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://lilithsangreal.com/p/transformer-from-zero-4/</guid>
      <description>从 0 开始学习 Transformer 拾遗：文章本身的与解释 首先感谢中科院计算所王子和先生提供的宝贵意见。
由于文章结构的关系，为了能够同时兼顾代码的真实和描述的通俗。我使用了一些可能会有一定误导性的举例说明。在这里做一些解释。
1. 关于 batch_size 在本系列上篇的 5.2.2. 使用向量化来提升效率，一节中。我使用了这样的描述：
 举个例子：若有 batch_size 批次，每批次 N 条的 Query，Key。 其计算完全可以组织成 (batch_size, N, seq_len_q, depth)点乘(batch_size, N, seq_len_q, depth)的转置，最终得到(batch_size, N, seq_len_q, seq_len_k) 形状的张量。
 熟悉深度学习的同学会发现，batch_size 这样的名字通常是用来描述一个批次的尺寸的，而并非描述批次数量。在 NLP 中，一次训练为一个批（batch），而一批若有 64 个句子，则 batch_size 为 64。在Transformer的实现中，它也是这样的含义。
举这个例子时，我是从最右维度为最小单位描述的，那么从最右数，第一维度是词嵌入（或特征向量）的维度，第二个维度是句子的维度，那第三个维度只能是批次的尺寸（而事实上，第三个维度N 是多头注意力的头数，是一个临时加入的维度，第四个维度才是批次的尺寸），描述这段文字的时候还没有介绍多头注意力，所以无奈之举将 batch_size 临时改了含义。
2. 关于多头注意力的计算图示带来的误导 这张图引用自国外的知名博客（在本系列上篇中提及）。显然为了能够更加直观易懂，这张图并非是基于 Transformer 真实的实现绘制的，而是按照多头注意力的原理绘制的。
但是一些读者看到这篇文章再结合代码可能会带来一些困惑。
在代码中，有两个关键的尺寸 depth 和 d_model。事实上，前者和后者的唯一关系是，前者刚好是后者拆成的 8 份之一，两者之间的转换只存在于拆分多头步骤，并没有其他任何映射关系（全连接网络）。
由于这张图的抽象化：词嵌入的矩阵看起来显然小于 8 个头的 K Q V；最后多头拼接的一个长长的结果$W^O$转换成一个 $Z$，读者可能会认为词嵌入的深度是 depth ，再经过一次全连接网络得到一个d_model 然后拆成多头，最后输出结果拼成 d_model 再经过一次全连接得到 depth，循环往复。</description>
    </item>
    
    <item>
      <title>从 0 开始学习 Transformer 番外：Transformer 如何穿梭时空？</title>
      <link>https://lilithsangreal.com/p/transformer-from-zero-3/</link>
      <pubDate>Sun, 15 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://lilithsangreal.com/p/transformer-from-zero-3/</guid>
      <description>从 0 开始学习 Transformer 番外：Transformer 如何穿梭时空？  从 0 开始学习 Transformer 番外：Transformer 如何穿梭时空？  1. 前言 2. Transformer 穿越时空了？ 3. 使用真值模拟输出配合前瞻遮挡    1. 前言 讲解 Transfomer 在训练阶段为何无需循环调用模型即可完成导师监督（teacher-forcing）法。讲解前瞻遮挡原理的精妙用法：通过一次正向传播，模拟模型逐个得到得到整个目标句子的预测过程。
2. Transformer 穿越时空了？ 首先，我们来看看 Transofrmer 是如何完成导师监督的（下面这是一张动图，依然来自Jay Alammar，有可能加载不出来，请参考原文The Decoder Side部分)：
这和本系列第二篇文章的 7.评估 部分是一致的：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25  ... for i in range(MAX_LENGTH): enc_padding_mask, combined_mask, dec_padding_mask = create_masks( encoder_input, output) # predictions.</description>
    </item>
    
    <item>
      <title>从 0 开始学习 Transformer 下篇：Transformer 训练与评估</title>
      <link>https://lilithsangreal.com/p/transformer-from-zero-2/</link>
      <pubDate>Fri, 13 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://lilithsangreal.com/p/transformer-from-zero-2/</guid>
      <description>从 0 开始学习 Transformer 下篇：Transformer 训练与评估  从 0 开始学习 Transformer 下篇：Transformer 训练与评估  1. 前言 2. 创造原训练集的编码表示  2.1. 数据下载与读取 2.2. 创建子词分词器 2.3. 数据处理   3. 损失函数设计 4. 优化器与学习率 5. 自回归原理 6. 训练  6.1. 超参数 6.2. 训练  6.2.1. 创建遮挡 6.2.2. 创建训练步骤及保存模型 6.2.3. 开始训练     7. 评估    1. 前言 在上一篇文章中我们已经描述了 Transformer 的整个模型搭建过程，并逐层逐行地解释了其正向传播的原理和细节。接下来，我们将着手定义优化训练的方式，处理语料，并最终使用搭建好的 Transformer 实现一个由葡萄牙语翻译至英语的翻译器。
为了训练一个由葡萄牙语翻译至英语的翻译器，首先来观察如何处理数据从而能够正确地输入我们已经设计好的 Tranformer 模型：
1 2 3 4 5 6 7  class Transformer(tf.</description>
    </item>
    
    <item>
      <title>从 0 开始学习 Transformer 上篇：Transformer 搭建与理解</title>
      <link>https://lilithsangreal.com/p/transformer-from-zero-1/</link>
      <pubDate>Mon, 02 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://lilithsangreal.com/p/transformer-from-zero-1/</guid>
      <description>从 0 开始学习 Transformer 上篇：Transformer 搭建与理解  从 0 开始学习 Transformer 上篇：Transformer 搭建与理解  1. 前言 2. 参考代码、文章及部分插图来源 3. 在开始前的推荐了解  3.1. 循环神经网络（RNN） 3.2. 基于编码-解码（encoder-decoder）的序列到序列（sequence2sequence）模型 3.3. 注意力机制 3.4. 词嵌入（Word Embedding）   4. 初探 Transformer 5. 基础算法和模块  5.1. 位置编码（Positional encoding） 5.2. 注意力机制  5.2.1. 计算步骤细节 5.2.2. 使用向量化来提升效率 5.2.3. 如何作为自注意力使用   5.3. 遮挡 Mask  5.3.1. 填充遮挡 5.3.2. 前瞻遮挡（look-ahead mask）   5.4. 多头注意力（Multi-head attention）  5.4.1. 代码分析   5.</description>
    </item>
    
  </channel>
</rss>
