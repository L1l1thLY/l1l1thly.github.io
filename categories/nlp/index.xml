<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on Lilith Sangreal</title>
    <link>https://lilithsangreal.com/categories/nlp/</link>
    <description>Recent content in NLP on Lilith Sangreal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://lilithsangreal.com/categories/nlp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>从 0 开始学习 Transformer 上篇：Transformer 搭建与理解</title>
      <link>https://lilithsangreal.com/p/test-chinese/</link>
      <pubDate>Wed, 02 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://lilithsangreal.com/p/test-chinese/</guid>
      <description>从 0 开始学习 Transformer 上篇：Transformer 搭建与理解  从 0 开始学习 Transformer 上篇：Transformer 搭建与理解  1. 前言 2. 参考代码、文章及部分插图来源 3. 在开始前的推荐了解  3.1. 循环神经网络（RNN） 3.2. 基于编码-解码（encoder-decoder）的序列到序列（sequence2sequence）模型 3.3. 注意力机制 3.4. 词嵌入（Word Embedding）   4. 初探 Transformer 5. 基础算法和模块  5.1. 位置编码（Positional encoding） 5.2. 注意力机制  5.2.1. 计算步骤细节 5.2.2. 使用向量化来提升效率 5.2.3. 如何作为自注意力使用   5.3. 遮挡 Mask  5.3.1. 填充遮挡 5.3.2. 前瞻遮挡（look-ahead mask）   5.4. 多头注意力（Multi-head attention）  5.4.1. 代码分析   5.</description>
    </item>
    
  </channel>
</rss>
