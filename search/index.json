[{"content":"iOS 性能指标工具集笔记 Vol.3 Xcode Metrics Organizer 简介 使用 Xcode - Window - Organizer 点击 Metrics 选择发布的 App 可以看到电池寿命、启动时间、无响应率、内存和磁盘写入的大盘统计数据。如下图所示：\n可以在右上角的下拉栏里专门看某一个型号设备的数据\n历史版本的内存最大占用和后台内存占用 历史版本的前台和后台电池消耗细节大盘统计数据 这些数据会按照网络、运算、显示、定位和其他来分类。\n启动时间 不响应用户的时间比例（Hang Rate） 磁盘写入量 ","date":"2022-09-05T11:19:31+08:00","permalink":"https://lilithsangreal.com/p/ios-%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87%E5%B7%A5%E5%85%B7%E9%9B%86%E7%AC%94%E8%AE%B0-vol.3-xcode-metrics-organizer/","title":"iOS 性能指标工具集笔记 Vol.3 Xcode Metrics Organizer"},{"content":"iOS 性能指标工具集笔记 Vol.2 XCTest 已有的Debug面板（可以Debug环境下看到CPU、内存、网络耗电量等等）和Instrument可以解决一些性能上的诊断问题：\n新增的 XCTest 工具则是以一种单元测试的形式来测量业务逻辑的性能指标。把一些业务逻辑包裹在一个用例里面，并设定预期的性能指标，如APP启动速度：\n如果没能达到预期的指标，则会产生一个用例失败：\n也可以编写测量内存表现等多个维度的单元测试：\n","date":"2022-09-03T22:29:08+08:00","permalink":"https://lilithsangreal.com/p/ios-%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87%E5%B7%A5%E5%85%B7%E9%9B%86%E7%AC%94%E8%AE%B0-vol.2-xctest/","title":"iOS 性能指标工具集笔记 Vol.2 XCTest"},{"content":"iOS 性能指标工具集笔记 Vol.1 MetricKit 综述 优秀的产品对于应用的功耗表现也有所要求。因此 iOS 在开发过程中会使用 Instruments、Energy Gauges 和 Profiling 来分析和改进应用的功耗表现。对于 Beta 测试，也可以利用 Instruments 进行分析。\n不考虑第三方框架，已经发布的 TestFlight / App Store 包，虽然我们仍有机会获取到一些崩溃日志（隐私日志里面的Crash日志等）。但这些很难让开发者量化分析应用的功耗表现，显然，我们需要更多数据。\n为此苹果在 2019 年伴随iOS13和 Xcode11 发布了一系列工具用于收集和分析用于手机功耗表现相关的数据，这些工具大量出现了 Metrics 这个关键词，我认为可以直接翻译为指标，其实苹果是给用户提供了更多可量化的指标数据及其相关工具、框架，这是第一批发布的指标工具：\nXCTest Metrics：以一种类似单元测试的方式，测量部分关键代码的性能和电池指标的框架 MetricKit：一个用于收集诸多用户电池和性能表现相关数据的框架 Xcode Metrics Organizer：一个统计大盘数据，可以看AppStore所有用户历史版本的性能表现 上图为苹果描述的应用场景。\n首批提供的指标如下：\nBattery Metrics：提供CPU、位置、显示、网络、蓝牙、多媒体和相册的功耗表现 Performance Metrics：提供挂起、磁盘、应用启动、内存和自定义间隔 在 2020 年，苹果再一次提升了这些指标工具的能力，发布了 Metric Kit2.0，拓展了三个指标集合 5:24\nCPU 指令：能够给出一天内究竟有多少指令执行了，极度精确地给出App究竟做了多少事 滚动故障：用户感受到的动画丢帧/不流畅占正常动画的比例，并进一步介绍如何用XCTest来排查与解决动画故障 APP 退出：给出一天内，APP究竟为什么退出了，退出了多少次。一个应用，除了正常退出，也会有很多情况被苹果干掉。本文在后面小节 进程退出原因MXAppExitMetric 进一步讨论。 除了指标集合，苹果还推出了可以帮助定位和修复问题的数据工具 MetricKit 2.0 Diagnostics，包含指标类型如下：\n挂起：采样App对用户长时间无反应（主线程阻塞等）时的堆栈 CPU 异常：采样高CPU占用时的堆栈 磁盘写入异常：采样总写入次数过多，异常过量写入发生时的堆栈（突破每日1GB） Crash：采样崩溃发生的原因，发生时的虚拟内存信息和堆栈 同时，升级了Xcode Metrics Organizer 分析性能指标的能力。\nMetricKit 如何工作 一旦为 App 接入了 MetricKit 之后，MetricKit 会在24小时之内收集性能指标，并在生成一份报告。\n同时，它也可以利用 mxSignPost 收集一些关键代码段的性能数据：\nMetrics 1.0 Battery Metrics 电池性能指标简介 那么 MetricKit 究竟收集了哪些数据和诊断呢？下图是一个指标报告的例子：\nProcessing Metrics 处理器指标 提供了一下CPU/GPU时间，可以利用这些时间找寻一些不应该有的UI渲染和CPU工作。可以用于量化比较不同的功能算法实现效率。\nLocation Metrics 定位指标 统计程序在前后台共计用了多久定位功能，定位功能的精确度有多高，防止定位持续空转或使用了过于精确的定位能力消耗额外电量。\nDisplay Metrics 显示指标 提供了平均像素亮度（APL），用于量化目前的UI设计会消耗多少电量。\nNetworking Metrics 网络指标 上传下载的字节数，网络的连通性。\nMetrics 1.0 Performance 体验表现指标简介 Hang Metrics 无响应指标 提供一个直方图，表示界面对用户没有响应的时间比例。如果有异常，一般是UI线程做事情太多了，需要移到其他线程。\nDisk Metrics 磁盘指标 提供了一些指标来判断是否有意料之外的磁盘写入。\nApplication Launch Metrics 应用指标 提供柱形图来量化启动和恢复的时间消耗，以便采取手段优化启动速度。\nMemory Metrics 内存指标 提供挂起（suspended）平均内存和内存峰值。\n使用 MetricKit 接入 MetricKit 假设需要获取性能指标的类是一个新的VC，接入方法如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 // // MetricViewController.m // UIPlayground // // Created by LilithSangreal on 2022/9/3. // Copyright © 2022 LilithSangreal. All rights reserved. // #import \u0026#34;MetricViewController.h\u0026#34; // 1. 引入 MetricKit 框架头文件 #import \u0026lt;MetricKit/MetricKit.h\u0026gt; @interface MetricViewController () \u0026lt;MXMetricManagerSubscriber\u0026gt; // 2. [可选] 声明一个 Manager 变量，主要用于移除订阅，如果在AppDelegate里面使用，可能不需要移除 @property (strong, nonatomic) MXMetricManager *metricManager; @end @implementation MetricViewController - (instancetype)init { self = [super init]; if (self) { // 3. 获取 MetricManager 单例 _metricManager = [MXMetricManager sharedManager]; // 4. 将订阅对象放入 Manager 中 [_metricManager addSubscriber:self]; } return self; } - (void)dealloc { // 5. [可选] 析构的时候需要移除观察者 [_metricManager removeSubscriber:self]; } 获取 MetricKit 指标 需要实现协议方法来获取指标信息，这个方法 iOS13 以上才可以使用。\n1 2 3 4 5 - (void)didReceiveMetricPayloads:(NSArray\u0026lt;MXMetricPayload *\u0026gt; *)payloads { for (MXMetricPayload *payload in payloads) { // ...可以上传服务器、存入本地、写入文件等等... } } 【MetricKit 2.0】获取 MXDiagnostic 信息 实现协议方法来获取诊断信息，这个方法 iOS14 以上才可以使用。\n接下来需要实现协议方法来获取指标信息，详细的介绍将会在后面的小节\n1 2 3 4 5 - (void)didReceiveDiagnosticPayloads:(NSArray\u0026lt;MXDiagnosticPayload *\u0026gt; *)payloads { for (MXDiagnosticPayload *payload in payloads) { // ...可以上传服务器、存入本地、写入文件等等... } } 分析关键代码段落的性能指标信息 mxSignPost 假设需要分析 doSomething 方法的功耗性能：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 - (void)viewDidLoad { [super viewDidLoad]; // 1. 创建 OSLog Handle，为它起个分类名吧 os_log_t doSomeThingHandle = [MXMetricManager makeLogHandleWithCategory:@\u0026#34;DoSomeThingCategory\u0026#34;]; os_signpost_id_t doSomeThingHandleID = os_signpost_id_generate(doSomeThingHandle); // 2. 标记关键代码段落开始，为它起个名字吧 MXSignpostIntervalBegin(doSomeThingHandle, doSomeThingHandleID, @\u0026#34;DoSomeThingAwesome\u0026#34;); [self doSomethingAwesome]; // 3. 标记关键代码段落结束 MXSignpostIntervalEnd(doSomeThingHandle, doSomeThingHandleID, @\u0026#34;DoSomeThingAwesome\u0026#34;); } 自测数据回包（需要真机） Xcode - Debug - Simulate MetricKit Payloads\n会产生一个虚假的数据回包，仅供验证是否正确接入框架。\nMXDiagnostic 使用 当我们已经了在 Metrix Payload 里面发现了性能问题存在的可能性，就需要借助 MXDiagnostic 来进一步定位问题。MXDiagnostic 的报告内容一一对应着 MXMetric 中统计的种种数量：\n报告中包含 MXCallStackTree，这是异常发生时的堆栈情况，可以通过反解符号进行问题定位。其他数据内容简介请参考前文综述。\n如何反解析 MXCallStackTree 符号 许多 iOS 开发者已经了熟悉如何利用 dSYM 反解 Crash 报告的符号：\n1 2 # 解析普通 Crash 文件符号的方法 atos -arch \u0026lt;架构如 arm64 \u0026gt; -o \u0026lt;dSYM 路径\u0026gt;/Contents/Resources/DWARF/\u0026lt;二进制文件名\u0026gt; -l \u0026lt;load address\u0026gt; \u0026lt;address to symbolicate\u0026gt; 如果已经已经有了产生 MXCallStackTree 报告对应的 dSYM 文件，和 MXCallStackTree 报告堆栈内容（如以 Json 格式打印）：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 ... { \u0026#34;binaryUUID\u0026#34; : \u0026#34;4746A9D9-...-71B86DEF7AE9\u0026#34;, \u0026#34;offsetIntoBinaryTextSegment\u0026#34; : 4377182208, \u0026#34;sampleCount\u0026#34; : 1, \u0026#34;subFrames\u0026#34; : [ { \u0026#34;binaryUUID\u0026#34; : \u0026#34;4746A9D9-...-71B86DEF7AE9\u0026#34;, \u0026#34;offsetIntoBinaryTextSegment\u0026#34; : 4377182208, \u0026#34;sampleCount\u0026#34; : 1, \u0026#34;subFrames\u0026#34; : [ { \u0026#34;binaryUUID\u0026#34; : \u0026#34;7C9C7851-...-9CD2DEB4B746\u0026#34;, \u0026#34;offsetIntoBinaryTextSegment\u0026#34; : 4665638912, \u0026#34;sampleCount\u0026#34; : 1, \u0026#34;binaryName\u0026#34; : \u0026#34;dyld\u0026#34;, \u0026#34;address\u0026#34; : 4665744612 } ], \u0026#34;binaryName\u0026#34; : \u0026#34;my_app\u0026#34;, \u0026#34;address\u0026#34; : 4530347636 } ... 对于其中一个 subFrames 我们需要取出其中 offsetIntoBinaryTextSegment 字段和 address 字段进行符号反解。\n1 atos -arch arm64e -i -l \u0026lt;16进制的 offsetIntoBinaryTextSegment\u0026gt; -o \u0026lt;dSYM 路径\u0026gt;/Contents/Resources/DWARF/\u0026lt;二进制文件名\u0026gt; \u0026lt;16进制的 address\u0026gt; 由于Json是一个嵌套结构，因此可以使用一个递归的Python脚本进行解析，以下是我的实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 把涉及到的所有 dSYM 放入一个 dict管理 dsym_dict def exec_cmd(cmd) -\u0026gt; object: return os.system(cmd) def get_symbol(d): if d[\u0026#39;binaryName\u0026#39;] in dsym_dict: exec_cmd(\u0026#34;atos -arch arm64e -i -l \u0026#34; + str(hex(d[\u0026#34;offsetIntoBinaryTextSegment\u0026#34;])) + \u0026#34; -o \u0026#34; + dsym_dict[d[\u0026#39;binaryName\u0026#39;]] + \u0026#34; \u0026#34; + str(hex(d[\u0026#34;address\u0026#34;]))) else: print(\u0026#34;[addr] \u0026#34; + d[\u0026#39;binaryName\u0026#39;] + \u0026#34;:\u0026#34; + str(hex(d[\u0026#34;offsetIntoBinaryTextSegment\u0026#34;]))) if \u0026#34;subFrames\u0026#34; in d: get_symbol(d[\u0026#34;subFrames\u0026#34;][0]) if __name__ == \u0026#39;__main__\u0026#39;: dic = read_json(bt_file) for index, thread in enumerate(dic[\u0026#34;callStackTree\u0026#34;][\u0026#39;callStacks\u0026#39;]): print(\u0026#34;===== thread: \u0026#34;, index, \u0026#34; ======\u0026#34;) get_symbol(thread[\u0026#39;callStackRootFrames\u0026#39;][0]) 应用场景：进程退出原因分析指标 MXAppExitMetric 应用在后台被杀死会影响用户的体验。如果用户有未保存的输入内容（同时应用开发者并没有进入后台的时候保存内容），就会因为进程被杀而导致丢失输入。同时进程被杀也会使用户下次切回APP时不得不再次启动，被迫欣赏闪屏页（和开屏广告）。\n但是由于进程异常退出/被 iOS 系统杀死的原因多种多样，往往很难定位问题。通过 MXAppExitMetric 可以获取软件退出的原因。分为前台退出和后台退出，这些指标都在 MXMetric payload 里面可以获取：\npayload.applicationExitMetrics.foregroundExitData payload.applicationExitMetrics.backgroundExitData 有一点不方便的是，这两个数据对象不包含直接的转换为 Json 或者 Dictionary 的方法，如果需要整体打印，可能需要一些运行时的方法。下面打印一个例子：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 in MXForegroundExitData: [8|0x8]_cumulativeNormalAppExitCount (unsigned long): 0 [16|0x10]_cumulativeMemoryResourceLimitExitCount (unsigned long): 0 [24|0x18]_cumulativeBadAccessExitCount (unsigned long): 0 [32|0x20]_cumulativeAbnormalExitCount (unsigned long): 0 [40|0x28]_cumulativeIllegalInstructionExitCount (unsigned long): 0 [48|0x30]_cumulativeAppWatchdogExitCount (unsigned long): 0 [56|0x38]_cumulativeCPUResourceLimitExitCount (unsigned long): 0 in MXBackgroundExitData: [8|0x8]_cumulativeNormalAppExitCount (unsigned long): 0 [16|0x10]_cumulativeMemoryResourceLimitExitCount (unsigned long): 0 [24|0x18]_cumulativeCPUResourceLimitExitCount (unsigned long): 0 [32|0x20]_cumulativeMemoryPressureExitCount (unsigned long): 1 [40|0x28]_cumulativeBadAccessExitCount (unsigned long): 0 [48|0x30]_cumulativeAbnormalExitCount (unsigned long): 0 [56|0x38]_cumulativeIllegalInstructionExitCount (unsigned long): 0 [64|0x40]_cumulativeAppWatchdogExitCount (unsigned long): 6 [72|0x48]_cumulativeSuspendedWithLockedFileExitCount (unsigned long): 0 [80|0x50]_cumulativeBackgroundTaskAssertionTimeoutExitCount (unsigned long): 1 [88|0x58]_cumulativeBackgroundFetchCompletionTimeoutExitCount (unsigned long): 0 [96|0x60]_cumulativeBackgroundURLSessionCompletionTimeoutExitCount (unsigned long): 0 可以看出他提供了多种多样的退出次数统计，而不同的退出一般发生的场景也不同：\n前/后台正常退出：cumulativeNormalAppExitCount\n前/后台异常退出：cumulativeAbnormalExitCount，这种退出一般发生在未捕获的OC异常、调用了中止方法或者其他导致中止信号的方法调用。\n前/后台WatchDog退出：cumulativeAppWatchdogExitCount，这种退出一般发生在启动/关闭或者响应系统事件耗费的时间过长，如果进入后台 / 前台耗费的时间过多，也会触发这种退出（applicationDidEnterBackground 和 applicationWillEnterForeground）\n前/后台内存超限：cumulativeMemoryResourceLimitExitCount\n前/后台CPU超限：cumulativeCPUResourceLimitExitCount\n前/后台异常访问退出（Crash）：cumulativeBadAccessExitCount，这种一般出现数组越界、野指针或者写入只读内存等等。\n前/后台非法指令退出（Crash）：cumulativeIllegalInstructionExitCount，调用一个没有正确配置好的函数指针会触发这种退出\n后台内存压力退出：cumulativeMemoryPressureExitCount，为了获得更多内存，iOS自动清理杀死了你的进程（目前实践上来看，不能每次都能在jetsam报告中找到对应报告）。此类退出通常无法完全避免，苹果的建议是保存好当前的视图层次结构，保存用户输入的草稿、播放的位置并尽可能应用UIKit State Restoration来减少影响。\n后台挂起时锁定文件退出：cumulativeSuspendedWithLockedFileExitCount，一般发生在正在挂起应用时候去写入数据库\n后台任务超时退出：cumulativeBackgroundTaskAssertionTimeoutExitCount，向系统申请执行后台任务的时间，但却一直没有调用 endBackgroundTask。\n但光有这些数量往往不能直接定位到问题。这时可以使用instrument或者Debug分析，也可以借助前文介绍的 MXDiagnostic 数据信息。如 MXCrashDiagnostic 可以拿到前/后台WatchDog退出发生时的堆栈。MXCPUExceptionDiagnostic 可以看到CPU超限的时候的堆栈。\n下一篇 iOS 性能指标工具集笔记 Vol.2 XCTest\n参考链接 WWDC 2019 - Improving Battery Life and Performance\nWWDC 2020 - What\u0026rsquo;s new in MetricKit\nWWDC 2020 - Eliminate animation hitches with XCTest\nWWDC 2020 - Why is my app getting killed?\nWWDC 2020 - Diagnose performance issues with the Xcode Organizer\nWWDC 2021 - Ultimate application performance survival guide\nWWDC 2021 - Understand and eliminate hangs from your app\nWWDC 2021 - Diagnose Power and Performance regressions in your app\nWWDC 2022 - Track down hangs with Xcode and on-device detection\n","date":"2022-08-26T19:46:00+08:00","permalink":"https://lilithsangreal.com/p/ios-%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87%E5%B7%A5%E5%85%B7%E9%9B%86%E7%AC%94%E8%AE%B0-vol.1-metrickit/","title":"iOS 性能指标工具集笔记 Vol.1 MetricKit"},{"content":"Protocol Buffers 编译器源码阅读 前言 Protocol Buffers （简称 pb/protobuf） 是谷歌公司开发的一款跨平台跨语言的结构数据序列化框架，可以使用 protobuf 的提供的编译器将定义好的数据模型（ proto文件 ）编译为各种语言，提供对应的特殊数据结构的序列化、反序列化能力。protobuf 性能好，语法简洁，广泛应用于 rpc 框架中，为其提供编解码的能力。\n除此之外，protobuf 还是一个优秀的 C++ 开源项目 Github，在社区里询问阅读什么项目提升自己的 C++ 编码能力时，也有很多人推荐阅读 protobuf 的源码。\n出于工作上的原因，最近我也对 protobuf 的编译器源码（Protocol buffers 3）进行一定阅读，发现它的代码整洁，逻辑清晰，设计也非常好。因此写一篇这样的文章记录一下阅读的一些收获。\n编译器相关知识 如果完全没有编译器相关的知识，那么阅读 protobuf 的源码是无从下手的。和寻常的编译器一样，protobuf 也可以分为编译器前端和后端，并完成了一个从源语言到目标语言的转换。对于传统编程语言来说，即是从源代码到机器直接执行的机器码的转换。\n如图所示，对于 protobuf 来说，源语言就是一个 proto 文件，定义了数据结构。而目标语言则是对应语言的源代码。以便于各种语言直接调用使用。\n目前 protobuf 支持的语言如下。也就是说，定义好的 proto 文件会转换为下列语言的源代码。\n对于编译器来说，前端主要完成的工作是词法分析，语法分析，语义分析。\n假设我们已经完成了预编译器的工作（一般包括去除空格，插入头文件，展开宏等等……事实上，根据不同的语言复杂程度，预编译器的工作也可能会非常复杂）。假设我们的语言非常简单，不需要引入什么头文件，展开宏等等，那么输入的目标语言将会被去除多余的空格和换行符。\n这样，对于编译器来说，目标语言只是一串连续的字母和特殊符号的序列，比如：\n1 whiletrue; 单个字母和符号一般不能表达什么含义，因此 词法分析 是将单词组合拆分成 token（单词）的形式， 如上述代码中的 while 和 true。经过词法分析，字母和特殊符号的序列就转化为了一串由单词形式组成的序列。\n而 语法分析 则是将这个单词的序列组织成一个抽象语法树（Abstract Syntax Tree，AST），后文会针对protobuf使用的语法分析方式介绍这一过程的实例。\n至此为止，其实编译器也只是将源语言，从一种不包含结构的序列形式转换成了含有结构的形式。这其中产生了任何的理解了吗？没有，至此位置，计算机只是对数据进行了一些处理工作，但并不知道我们的代码做了什么事情。\n但是组织一棵树的形式，则为语言初步理解源代码做了什么，做的事情是否合法提供了条件。编译器会反复阅读这棵树，根据这棵树的结构来分析语义，来判断程序员输入的东西是否合理。在这个过程中，编译器会为AST的每个节点填写一些属性。这就是 语义分析。\nLL(1) 文法 ","date":"2022-04-29T00:00:00Z","image":"https://lilithsangreal.com/p/google-protocol-buffers-notes/attachments/2022-04-29-16-27-11_hu027c5933656146fc15554de25a0d1438_144723_120x120_fill_box_smart1_3.png","permalink":"https://lilithsangreal.com/p/google-protocol-buffers-notes/","title":"Protocol Buffers 编译器源码阅读"},{"content":"从 0 开始学习 Transformer 拾遗：文章本身的与解释 首先感谢中科院计算所王子和先生提供的宝贵意见。\n由于文章结构的关系，为了能够同时兼顾代码的真实和描述的通俗。我使用了一些可能会有一定误导性的举例说明。在这里做一些解释。\n1. 关于 batch_size 在本系列上篇的 5.2.2. 使用向量化来提升效率，一节中。我使用了这样的描述：\n举个例子：若有 batch_size 批次，每批次 N 条的 Query，Key。 其计算完全可以组织成 (batch_size, N, seq_len_q, depth)点乘(batch_size, N, seq_len_q, depth)的转置，最终得到(batch_size, N, seq_len_q, seq_len_k) 形状的张量。\n熟悉深度学习的同学会发现，batch_size 这样的名字通常是用来描述一个批次的尺寸的，而并非描述批次数量。在 NLP 中，一次训练为一个批（batch），而一批若有 64 个句子，则 batch_size 为 64。在Transformer的实现中，它也是这样的含义。\n举这个例子时，我是从最右维度为最小单位描述的，那么从最右数，第一维度是词嵌入（或特征向量）的维度，第二个维度是句子的维度，那第三个维度只能是批次的尺寸（而事实上，第三个维度N 是多头注意力的头数，是一个临时加入的维度，第四个维度才是批次的尺寸），描述这段文字的时候还没有介绍多头注意力，所以无奈之举将 batch_size 临时改了含义。\n2. 关于多头注意力的计算图示带来的误导 这张图引用自国外的知名博客（在本系列上篇中提及）。显然为了能够更加直观易懂，这张图并非是基于 Transformer 真实的实现绘制的，而是按照多头注意力的原理绘制的。\n但是一些读者看到这篇文章再结合代码可能会带来一些困惑。\n在代码中，有两个关键的尺寸 depth 和 d_model。事实上，前者和后者的唯一关系是，前者刚好是后者拆成的 8 份之一，两者之间的转换只存在于拆分多头步骤，并没有其他任何映射关系（全连接网络）。\n由于这张图的抽象化：词嵌入的矩阵看起来显然小于 8 个头的 K Q V；最后多头拼接的一个长长的结果$W^O$转换成一个 $Z$，读者可能会认为词嵌入的深度是 depth ，再经过一次全连接网络得到一个d_model 然后拆成多头，最后输出结果拼成 d_model 再经过一次全连接得到 depth，循环往复。\n事实上并不是这样，在多个编码器解码器层中，从一开始的词嵌入的深度就是 d_model，经过一次全连接网络得到映射后的KQV——深度同样是 d_model，然后拆成多头，计算后拼接恰好也是d_model。再回到一开始，循环往复。\n也就是说，每一层注意力机制外，只有一层全连接网络。\n","date":"2020-01-17T00:00:00Z","image":"https://lilithsangreal.com/p/transformer-from-zero-4/the_transformer_hu9dc15172e42e840ef18a078a4efb196f_44824_120x120_fill_box_smart1_3.png","permalink":"https://lilithsangreal.com/p/transformer-from-zero-4/","title":"从 0 开始学习 Transformer 拾遗：文章本身的与解释"},{"content":"从 0 开始学习 Transformer 番外：Transformer 如何穿梭时空？ 从 0 开始学习 Transformer 番外：Transformer 如何穿梭时空？ 1. 前言 2. Transformer 穿越时空了？ 3. 使用真值模拟输出配合前瞻遮挡 1. 前言 讲解 Transfomer 在训练阶段为何无需循环调用模型即可完成导师监督（teacher-forcing）法。讲解前瞻遮挡原理的精妙用法：通过一次正向传播，模拟模型逐个得到得到整个目标句子的预测过程。\n2. Transformer 穿越时空了？ 首先，我们来看看 Transofrmer 是如何完成导师监督的（下面这是一张动图，依然来自Jay Alammar，有可能加载不出来，请参考原文The Decoder Side部分)：\n这和本系列第二篇文章的 7.评估 部分是一致的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ... for i in range(MAX_LENGTH): enc_padding_mask, combined_mask, dec_padding_mask = create_masks( encoder_input, output) # predictions.shape == (batch_size, seq_len, vocab_size) predictions, attention_weights = transformer(encoder_input, output, False, enc_padding_mask, combined_mask, dec_padding_mask) # 从 seq_len 维度选择最后一个词 predictions = predictions[: ,-1:, :] # (batch_size, 1, vocab_size) predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32) # 如果 predicted_id 等于结束标记，就返回结果 if predicted_id == tokenizer_en.vocab_size+1: return tf.squeeze(output, axis=0), attention_weights # 连接 predicted_id 与输出，作为解码器的输入传递到解码器。 output = tf.concat([output, predicted_id], axis=-1) ... 代码和动图过程一致。想要预测I am a student。首先我们将其处理成：\u0026lt;SOS\u0026gt; I am a student 作为解码器端的输入。而我们预期需要的得到的输出是 I am a student \u0026lt;EOS\u0026gt;。\n显然，第一次传输给解码器端的输入，只是一个开始符号：\n\u0026lt;SOS\u0026gt;\n此时预测出的是第一个单词:\nI\n然后，将预测出的第一个单词结合原输入一起输入解码器端：\n\u0026lt;SOS\u0026gt; I\n得到新的输出:\nI am\n这时我们将最后一个单词 am 结合上一步输入一起输入解码器端：\n\u0026lt;SOS\u0026gt; I am\n得到新的输出：\n\u0026lt;SOS\u0026gt; I am a\n反复此过程，直到新的输出最后一个单词代表结束符号 \u0026lt;EOS\u0026gt;。返回上一步输出（上步输出不包含 \u0026lt;EOS\u0026gt;）。\n显然，每一步预测都需要依赖上一步预测的结果。\n而看过前两篇文章的聪明网友一定发现了，我们在训练过程中，并没有循环调用这个步骤，而是直接将整个句子输入给编码器端。\n也就是说，训练过程并没有循环依赖前一次输出的步骤。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ... # 一个训练步骤 @tf.function(input_signature=train_step_signature) def train_step(inp, tar): tar_inp = tar[:, :-1] tar_real = tar[:, 1:] enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp) with tf.GradientTape() as tape: predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask) loss = loss_function(tar_real, predictions) ... # 训练多个步骤 # inp -\u0026gt; portuguese, tar -\u0026gt; english for (batch, (inp, tar)) in enumerate(train_dataset): train_step(inp, tar) ... Transformer 是如何在训练阶段通过一次预测过程就完成了本应循环一个句子长度那么多次的预测过程呢？莫非 Transformer 穿越时空了吗？\n3. 使用真值模拟输出配合前瞻遮挡 因为后续的计算，如残差、拆成多头、编码器解码器注意力、全连接网络等等，都不会改变前瞻遮挡对于原输入句子和输出句子的意义（不放心的同学可以结合代码追踪运行一下），所以将解码器端无伤大雅地简化为一个带有前瞻遮挡的自注意力机制。\n假设我们已经预测出了 I am a，需要预测出 I am a Student\n那么输入序列将是 \u0026lt;SOS\u0026gt; I am a。其表示为 (seq_len, depth) （因为只考虑一个句子和单头，所以省略了前置维度(batch, head_num)）。\n对于注意力机制，Key 和 Query 都是输入序列。显然，其自注意力权重(seq_len, seq_len)示意图如下：\n而生成的前瞻遮挡(seq_len, seq_len)示意图如下：\n由于前瞻遮挡的存在，最终注意力权重将只留下左下标为 0 的深蓝色部分。\n这样的注意力矩阵乘上和 \u0026lt;SOS\u0026gt; I am a 依次对应的 Value (seq_len, depth)：\n得到的结果(seq_len, depth)，便应该是 I am a Student 的表示。\n观察此乘法的过程（注意力权重点乘Value），由于前瞻遮挡的存在，这输出中的 I 实际上只来自 \u0026lt;SOS\u0026gt; 。而 am 则来自 \u0026lt;SOS\u0026gt; I 的加权求和。同样的， a 来自 \u0026lt;SOS\u0026gt; I am 的加权求和。\n如此巧妙！不需要反复调用Tranformer，显然，由于前瞻遮挡，注意力权重求和的过程已经潜在地完成了每一步导师监督（teacher-forcing）法的过程。\n在预测过程中，由于我们没有目标序列的真值，我们无法提前知道结束符号 EOS 前每一步的输出。但训练过程中，我们早已经拥有了 EOS 前所有的真值，将真值作为模型 “本应该” 的输出序列，再输入解码器层，前瞻遮挡将潜在地一次完成每一步导师监督（teacher-forcing）法的过程。\n","date":"2019-12-15T00:00:00Z","image":"https://lilithsangreal.com/p/transformer-from-zero-3/the_transformer_hu9dc15172e42e840ef18a078a4efb196f_44824_120x120_fill_box_smart1_3.png","permalink":"https://lilithsangreal.com/p/transformer-from-zero-3/","title":"从 0 开始学习 Transformer 番外：Transformer 如何穿梭时空？"},{"content":"从 0 开始学习 Transformer 下篇：Transformer 训练与评估 从 0 开始学习 Transformer 下篇：Transformer 训练与评估 1. 前言 2. 创造原训练集的编码表示 2.1. 数据下载与读取 2.2. 创建子词分词器 2.3. 数据处理 3. 损失函数设计 4. 优化器与学习率 5. 自回归原理 6. 训练 6.1. 超参数 6.2. 训练 6.2.1. 创建遮挡 6.2.2. 创建训练步骤及保存模型 6.2.3. 开始训练 7. 评估 1. 前言 在上一篇文章中我们已经描述了 Transformer 的整个模型搭建过程，并逐层逐行地解释了其正向传播的原理和细节。接下来，我们将着手定义优化训练的方式，处理语料，并最终使用搭建好的 Transformer 实现一个由葡萄牙语翻译至英语的翻译器。\n为了训练一个由葡萄牙语翻译至英语的翻译器，首先来观察如何处理数据从而能够正确地输入我们已经设计好的 Tranformer 模型：\n1 2 3 4 5 6 7 class Transformer(tf.keras.Model): ... def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask): ... 只摘取模型的调用 call 部分，可以看出 Transformer 需要的输入：\ninp：输入序列，这里需要的是源语言（葡萄牙语）的编码表示。（嵌入表示将在编码器中完成） tar：目标序列，这里需要的是目标语言（英语）的编码表示。（嵌入表示将在编码器中完成） training：布尔量，规定模型是否可以训练。 enc_padding_mask：编码器，填充遮挡。 look_ahead_mask：前瞻遮挡。两个遮挡将在后面详细描述。 dec_padding_mask：解码器，填充遮挡。 由此，我们知道，为了达成目的，我们需要完成以下几个步骤：\n创造原训练集（输入句子和目标句子）的嵌入表示 为我们的 Transformer 设计优化器和损失函数 根据情况创造填充遮挡 为了实现自回归创建前瞻遮挡 将数据输入进行训练 最终对训练好的模型进行评估 2. 创造原训练集的编码表示 2.1. 数据下载与读取 参考 Tensorflow 的官方教程，我们同样使用 TFDS 来进行数据的下载和载入。（应首先在本机环境或虚拟环境中安装 tensorflow_datasets 模块。\n1 2 3 4 5 import tensorflow_datasets as tfds examples, metadata = tfds.load(\u0026#39;ted_hrlr_translate/pt_to_en\u0026#39;, with_info=True, as_supervised=True) train_examples, val_examples = examples[\u0026#39;train\u0026#39;], examples[\u0026#39;validation\u0026#39;] 第一行代码会访问用户目录下（Windows和Unix系系统各有不同，请参考官方文档）是否已经下载好了葡萄牙翻译至英文翻译器所需的数据集，如果不存在，则会自动下载。第二行，则将其自动转换为训练集合和测试集合两个 tf.data.Dataset 实例。\n2.2. 创建子词分词器 tfds 独立于 Tensorflow，是专门用来管理和下载一些成熟的数据集的Python库。但其中有很多我认为通用性很强的函数。比如子词分词器：\n1 2 3 4 5 tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus( (en.numpy() for pt, en in train_examples), target_vocab_size=2**13) tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus( (pt.numpy() for pt, en in train_examples), target_vocab_size=2**13) 上方代码分别创建了两个子词分词器，分别读取了训练集合中的全部英文和葡萄牙文，并基于这些大段的文字形成了子词分词器。\n子词分词器的作用是将输入句子中的每一个单词编码为一个独一无二的数字，如果出现了子词分词器不能识别的新单词，那么就将其打散成多个可以识别的子词来编码成数字。\n同样的，分词器也可以将用数字表示的句子重新转换回原有的句子。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 sample_string = \u0026#39;Transformer is awesome.\u0026#39; tokenized_string = tokenizer_en.encode(sample_string) print (\u0026#39;Tokenized string is {}\u0026#39;.format(tokenized_string)) # Tokenized string is [7915, 1248, 7946, 7194, 13, 2799, 7877] original_string = tokenizer_en.decode(tokenized_string) print (\u0026#39;The original string: {}\u0026#39;.format(original_string)) # The original string: Transformer is awesome. assert original_string == sample_string # 分词器转换回的句子和原始句子一定是相同的。 2.3. 数据处理 为了方便后期使用，编写一个将编码后句子加上开始标记和结束标记。利用 tf.data.Dataset 的 map 功能来批量完成这一任务。首先需要定义一个函数：\n1 2 3 4 5 6 7 8 def encode(lang1, lang2): lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode( lang1.numpy()) + [tokenizer_pt.vocab_size+1] lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode( lang2.numpy()) + [tokenizer_en.vocab_size+1] return lang1, lang2 显然这里我们使用了原生 Python 编写这个函数，这样的函数是不能为 map 所用的。我们需要使用 tf.py_function 将这个函数转换为计算图。（此函数可以将原生 Python 编写的计算过程转换为 Tensorflow 流程控制的计算图，详情请参考 py_function)\n1 2 3 def tf_encode(pt, en): return tf.py_function(encode, [pt, en], [tf.int64, tf.int64]) # 第一个参数是包装的函数，第二个参数是输入的参数列表，第三个参数是输出的数据类型 于是，我们可以给训练数据集和验证数据集中每个句子加上开始标记和结束标记：\n1 2 train_dataset = train_examples.map(tf_encode) val_dataset = val_examples.map(tf_encode) 为了能够让这个模型较小，我们只使用句子短于 40 个单词的句子来作为输入数据。这里利用 tf.data.Dataset 的 filter 过滤器功能来快速筛选出需要的数据。\n为了使用 filter，首先要定义一个过滤器函数。这是一个布尔函数，如果一条数据符合要求，则返回真，否则返回假。显然，对于葡萄牙句子翻译至英文句子数据集，我们要筛选出所有成对相同意思的句子，并且两条句子都短于 40 个单词（编码后并加上了开始和终结标记后的长度）。\n1 2 3 def filter_max_length(x, y, max_length=MAX_LENGTH): return tf.logical_and(tf.size(x) \u0026lt;= max_length, tf.size(y) \u0026lt;= max_length) 类似的，对数据集进行筛选：\n1 train_dataset = train_dataset.filter(filter_max_length) 我们已经知道， 输入给 Transformer 的句子通常不会单句地输入，而是把句子叠成一批输入。将一批有长有短的句子叠成一批，需要将较短的句子补 0 使其长度匹配当前一批中最长的句子。\n1 2 3 4 5 6 7 # 将数据集缓存到内存中以加快读取速度。 train_dataset = train_dataset.cache() # shuffle 函数定义一个随机方式，首先定义一个缓存大小，取一部分数据放入缓存（BUFFER_SIZE大小），然后进行随机洗牌，最后从缓存中取。显然，若想实现全数据集的完美随机，需要让缓存的大小大于等于整个数据集。 # 首先将数据进行随机打散之后，对较短的数据进行填充。 train_dataset = train_dataset.shuffle(BUFFER_SIZE).padded_batch( BATCH_SIZE, padded_shapes=([-1], [-1])) train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE) 显然，验证集合也需要进行类似的处理操作（验证操作无需随机）。\n1 2 val_dataset = val_dataset.filter(filter_max_length).padded_batch( BATCH_SIZE, padded_shapes=([-1], [-1])) 取出一个数据看一看：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 pt_batch, en_batch = next(iter(val_dataset)) \u0026#34;\u0026#34;\u0026#34; pt_batch: (\u0026lt;tf.Tensor: id=207688, shape=(64, 40), dtype=int64, numpy= array([[8214, 1259, 5, ..., 0, 0, 0], [8214, 299, 13, ..., 0, 0, 0], [8214, 59, 8, ..., 0, 0, 0], ..., [8214, 95, 3, ..., 0, 0, 0], [8214, 5157, 1, ..., 0, 0, 0], [8214, 4479, 7990, ..., 0, 0, 0]])\u0026gt;, en_batch: \u0026lt;tf.Tensor: id=207689, shape=(64, 40), dtype=int64, numpy= array([[8087, 18, 12, ..., 0, 0, 0], [8087, 634, 30, ..., 0, 0, 0], [8087, 16, 13, ..., 0, 0, 0], ..., [8087, 12, 20, ..., 0, 0, 0], [8087, 17, 4981, ..., 0, 0, 0], [8087, 12, 5453, ..., 0, 0, 0]])\u0026gt;) \u0026#34;\u0026#34;\u0026#34; 3. 损失函数设计 损失函数的设计较为简单，需要考虑输出的句子和真正的目标句子是否为同一句子。只需要使用一个交叉熵函数。有一点需要注意，由上一章数据处理可以看出，数据中含有大量的填充（补0），这些填充不能作为真正的输入来考虑，因此在损失函数的计算中，需要将这些部分屏蔽掉。\n1 2 3 4 5 6 7 8 9 10 11 12 13 loss_object = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True, reduction=\u0026#39;none\u0026#39;) def loss_function(real, pred): # 对于mask，如果编码句子中出现了值为 0 的数据，则将其置 0 mask = tf.math.logical_not(tf.math.equal(real, 0)) # 输出句子和真正的句子计算交叉熵 loss_ = loss_object(real, pred) # 将无效的交叉熵删除 mask = tf.cast(mask, dtype=loss_.dtype) loss_ *= mask # 返回平均值 return tf.reduce_mean(loss_) 同时，定义两个指标用于展示训练过程中的模型变化：\n1 2 3 train_loss = tf.keras.metrics.Mean(name=\u0026#39;train_loss\u0026#39;) train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy( name=\u0026#39;train_accuracy\u0026#39;) 4. 优化器与学习率 Transformer 使用 Adam 优化器，其 $\\beta_1$ 为 0.9， $\\beta_2$ 为0.98, $\\epsilon$ 为 $10^{-9}$。其学习率随着训练的进程变化：\n其中，这个 warmup_step 设定为 4000。如此设计，学习率随着训练（Train Step）的变化就如下图所示\n学习率的变化，我们通过继承 tf.keras.optimizers.schedules.LearningRateSchedule来实现。顾名思义，这个类会创建一个可序列化的学习率衰减（也可能增加）时间表：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule): def __init__(self, d_model, warmup_steps=4000): super(CustomSchedule, self).__init__() self.d_model = d_model self.d_model = tf.cast(self.d_model, tf.float32) self.warmup_steps = warmup_steps def __call__(self, step): #　这个时间表被调用时，按照 step 返回学习率 arg1 = tf.math.rsqrt(step) arg2 = step * (self.warmup_steps ** -1.5) return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2) 优化器便可以方便地使用这个类的实例改变学习率优化。\n1 2 3 4 learning_rate = CustomSchedule(d_model) optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9) 5. 自回归原理 至今位置，我们已经拥有了 Transformer 的完整模型，数据输入和优化器。\n但显然，Transformer 和 传统的 RNN 按时序依次读取输入和输出的训练方式“看起来”不同——它一次输入整个句子。而 encoder-decoder 架构是自回归的：通过上一步产生的符号和这一步的输入来预测这一步的输出。开始训练之前，需要了解 Transformer 是如何实现自回归的。\nTranformer 使用导师监督（teacher-forcing）法，即在预测过程中无论模型在当前时间步骤下预测出什么，teacher-forcing 方法都会将真实的输出传递到下一个时间步骤上。\n当 transformer 预测每个词时，自注意力（self-attention）功能使它能够查看输入序列中前面的单词，从而更好地预测下一个单词。为了仅能让其查看输入序列中前面的单词，则需要前瞻遮挡来屏蔽后方的单词。\n也就是说，若输入一个葡萄牙文句子，Tranformer 将第一次仅预测出英文句子的第一个单词，然后再次基础上依次预测第二个，第三个。\n而训练过程也应该模拟这样的预测过程，每次仅增加一个目标序列的单词。\n因此，我们将目标句子改写成两种：\n原目标句子：sentence = \u0026ldquo;SOS A lion in the jungle is sleeping EOS\u0026rdquo;\n改写为：\ntar_inp = \u0026ldquo;SOS A lion in the jungle is sleeping\u0026rdquo;\ntar_real = \u0026ldquo;A lion in the jungle is sleeping EOS\u0026rdquo;\n（SOS 和 EOS 是开始标记和结束标记。）\n真正输入给 Decoder 部分的是前者，配合前瞻遮挡它将模拟逐个单词产生的模型历史预测。而后者，则代表着模型当前步骤应该依次预测出的单词序列。很显然，他们应该仅仅只有一个单词的位移。\n6. 训练 6.1. 超参数 Transformer 的基础模型使用的数值为：num_layers=6，d_model = 512，dff = 2048。\n1 2 3 4 5 6 7 8 num_layers = 4 d_model = 128 dff = 512 num_heads = 8 input_vocab_size = tokenizer_pt.vocab_size + 2 target_vocab_size = tokenizer_en.vocab_size + 2 dropout_rate = 0.1 6.2. 训练 1 2 3 4 5 transformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input=input_vocab_size, pe_target=target_vocab_size, rate=dropout_rate) 6.2.1. 创建遮挡 首先要对输入数据（原始句子和目标句子）创建填充遮挡（填充了 0 的位置标记为 1，其余部分标记为 0，这里与损失函数的部分刚好相反）。 对于编码器解码器结构，当编码器预测后方的单词，只使用前方已经预测出的单词。为了实现这一效果，需要使用前瞻遮挡。 无论哪种遮挡，0 标记着保留的部分，1 标记着要遮挡的部分。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 这里 inp 和 tar 都是来自第 2 章的编码后数据，形状显然为 (batch_size, len) def create_masks(inp, tar): # 编码器填充遮挡，编码器自注意力时使用，在自注意力编码时排除掉没有含义的填充 enc_padding_mask = create_padding_mask(inp) # 在解码器的第二个注意力模块使用。 # 该填充遮挡用于遮挡编码器的输出，其输出输送给解码器使用，排除掉没有含义的填充 dec_padding_mask = create_padding_mask(inp) # 在解码器的第一个注意力模块使用。 # 遮挡（mask）解码器获取到的输入的后续标记（future tokens）。 # 自然，填充的 padding 也不能忘记考虑，所以把两个遮挡合在一起两全其美 look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1]) dec_target_padding_mask = create_padding_mask(tar) combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask) return enc_padding_mask, combined_mask, dec_padding_mask 6.2.2. 创建训练步骤及保存模型 为了保存模型，需要创建一个检查点管理器，在需要时使用此管理器来保存模型：\n1 2 3 4 5 6 7 8 9 10 11 checkpoint_path = \u0026#34;./checkpoints/train\u0026#34; ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer) ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5) # 如果检查点存在，则恢复最新的检查点。 if ckpt_manager.latest_checkpoint: ckpt.restore(ckpt_manager.latest_checkpoint) print (\u0026#39;Latest checkpoint restored!!\u0026#39;) 根据 5. 节的描述，将对目标序列进行调整和创建遮挡。最终实现训练过程。\n在TF2.0中，由于使用了 eager excution 导致的性能下降，将使用@tf.function 装饰器将代码转换为传统的计算图提升性能。但这种转换并非完全智能，若没有良好的限制，则会因为输入 Tensor 的变化导致无法复用已有的计算图，导致冗余的转换。详情请参考Graph Execution 模式。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # 该 @tf.function 将追踪-编译 train_step 并将其转换为计算图，以便更快地执行。 # 该函数专用于参数张量的精确形状。为了避免由于可变序列长度或可变批次大小（最后一批次较小）导致的多次冗余转换 # 使用 input_signature 指定更多的通用形状。 train_step_signature = [ tf.TensorSpec(shape=(None, None), dtype=tf.int64), tf.TensorSpec(shape=(None, None), dtype=tf.int64), ] @tf.function(input_signature=train_step_signature) def train_step(inp, tar): tar_inp = tar[:, :-1] tar_real = tar[:, 1:] enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp) with tf.GradientTape() as tape: predictions, _ = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask) loss = loss_function(tar_real, predictions) gradients = tape.gradient(loss, transformer.trainable_variables) optimizer.apply_gradients(zip(gradients, transformer.trainable_variables)) train_loss(loss) train_accuracy(tar_real, predictions) 6.2.3. 开始训练 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 EPOCHS = 20 for epoch in range(EPOCHS): start = time.time() train_loss.reset_states() train_accuracy.reset_states() # inp -\u0026gt; portuguese, tar -\u0026gt; english for (batch, (inp, tar)) in enumerate(train_dataset): train_step(inp, tar) if batch % 50 == 0: print (\u0026#39;Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\u0026#39;.format( epoch + 1, batch, train_loss.result(), train_accuracy.result())) if (epoch + 1) % 5 == 0: ckpt_save_path = ckpt_manager.save() print (\u0026#39;Saving checkpoint for epoch {} at {}\u0026#39;.format(epoch+1, ckpt_save_path)) print (\u0026#39;Epoch {} Loss {:.4f} Accuracy {:.4f}\u0026#39;.format(epoch + 1, train_loss.result(), train_accuracy.result())) print (\u0026#39;Time taken for 1 epoch: {} secs\\n\u0026#39;.format(time.time() - start)) 效果如下：\n1 2 3 4 5 6 Epoch 1 Batch 0 Loss 4.4721 Accuracy 0.0000 Epoch 1 Batch 50 Loss 4.2211 Accuracy 0.0076 Epoch 1 Batch 100 Loss 4.1943 Accuracy 0.0173 Epoch 1 Batch 150 Loss 4.1539 Accuracy 0.0205 Epoch 1 Batch 200 Loss 4.0675 Accuracy 0.0221 ... 7. 评估 以下步骤用于评估：\n用葡萄牙语分词器（tokenizer_pt）编码输入语句。此外，添加开始和结束标记，这样输入就与模型训练的内容相同。这是编码器输入。 解码器输入为 start token == tokenizer_en.vocab_size。 计算填充遮挡和前瞻遮挡。 解码器通过查看编码器输出和它自身的输出（自注意力）给出预测。 选择最后一个词并计算它的 argmax。将预测的词连接到解码器输入，然后传递给解码器。在这种方法中，解码器根据它预测的之前的词预测下一个。 评估函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def evaluate(inp_sentence): start_token = [tokenizer_pt.vocab_size] end_token = [tokenizer_pt.vocab_size + 1] # 输入语句是葡萄牙语，增加开始和结束标记 inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token encoder_input = tf.expand_dims(inp_sentence, 0) # 因为目标是英语，输入 transformer 的第一个词应该是 # 英语的开始标记。 decoder_input = [tokenizer_en.vocab_size] output = tf.expand_dims(decoder_input, 0) for i in range(MAX_LENGTH): enc_padding_mask, combined_mask, dec_padding_mask = create_masks( encoder_input, output) # predictions.shape == (batch_size, seq_len, vocab_size) predictions, attention_weights = transformer(encoder_input, output, False, enc_padding_mask, combined_mask, dec_padding_mask) # 从 seq_len 维度选择最后一个词 predictions = predictions[: ,-1:, :] # (batch_size, 1, vocab_size) predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32) # 如果 predicted_id 等于结束标记，就返回结果 if predicted_id == tokenizer_en.vocab_size+1: return tf.squeeze(output, axis=0), attention_weights # 连接 predicted_id 与输出，作为解码器的输入传递到解码器。 output = tf.concat([output, predicted_id], axis=-1) return tf.squeeze(output, axis=0), attention_weights 可视化注意力:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def plot_attention_weights(attention, sentence, result, layer): fig = plt.figure(figsize=(16, 8)) sentence = tokenizer_pt.encode(sentence) attention = tf.squeeze(attention[layer], axis=0) for head in range(attention.shape[0]): ax = fig.add_subplot(2, 4, head+1) # 画出注意力权重 ax.matshow(attention[head][:-1, :], cmap=\u0026#39;viridis\u0026#39;) fontdict = {\u0026#39;fontsize\u0026#39;: 10} ax.set_xticks(range(len(sentence)+2)) ax.set_yticks(range(len(result))) ax.set_ylim(len(result)-1.5, -0.5) ax.set_xticklabels( [\u0026#39;\u0026lt;start\u0026gt;\u0026#39;]+[tokenizer_pt.decode([i]) for i in sentence]+[\u0026#39;\u0026lt;end\u0026gt;\u0026#39;], fontdict=fontdict, rotation=90) ax.set_yticklabels([tokenizer_en.decode([i]) for i in result if i \u0026lt; tokenizer_en.vocab_size], fontdict=fontdict) ax.set_xlabel(\u0026#39;Head {}\u0026#39;.format(head+1)) plt.tight_layout() plt.show() 单句子测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def translate(sentence, plot=\u0026#39;\u0026#39;): result, attention_weights = evaluate(sentence) predicted_sentence = tokenizer_en.decode([i for i in result if i \u0026lt; tokenizer_en.vocab_size]) print(\u0026#39;Input: {}\u0026#39;.format(sentence)) print(\u0026#39;Predicted translation: {}\u0026#39;.format(predicted_sentence)) if plot: plot_attention_weights(attention_weights, sentence, result, plot) translate(\u0026#34;este é um problema que temos que resolver.\u0026#34;) print (\u0026#34;Real translation: this is a problem we have to solve .\u0026#34;) 效果：\n1 2 3 Input: este é um problema que temos que resolver. Predicted translation: this is a problem that we have to solve .... now . Real translation: this is a problem we have to solve . ","date":"2019-12-13T00:00:00Z","image":"https://lilithsangreal.com/p/transformer-from-zero-2/the_transformer_hu9dc15172e42e840ef18a078a4efb196f_44824_120x120_fill_box_smart1_3.png","permalink":"https://lilithsangreal.com/p/transformer-from-zero-2/","title":"从 0 开始学习 Transformer 下篇：Transformer 训练与评估"},{"content":"从 0 开始学习 Transformer 上篇：Transformer 搭建与理解 从 0 开始学习 Transformer 上篇：Transformer 搭建与理解 1. 前言 2. 参考代码、文章及部分插图来源 3. 在开始前的推荐了解 3.1. 循环神经网络（RNN） 3.2. 基于编码-解码（encoder-decoder）的序列到序列（sequence2sequence）模型 3.3. 注意力机制 3.4. 词嵌入（Word Embedding） 4. 初探 Transformer 5. 基础算法和模块 5.1. 位置编码（Positional encoding） 5.2. 注意力机制 5.2.1. 计算步骤细节 5.2.2. 使用向量化来提升效率 5.2.3. 如何作为自注意力使用 5.3. 遮挡 Mask 5.3.1. 填充遮挡 5.3.2. 前瞻遮挡（look-ahead mask） 5.4. 多头注意力（Multi-head attention） 5.4.1. 代码分析 5.5. 点式前馈网络（Point wise feed forward network） 6. 编码器解码器 6.1. 编码器层 6.2. 解码器层 6.3. 编码器 6.4. 解码器 7. 创建 Transformer 8. 小结 1. 前言 本文章结合代码讲解如何使用 Tensorflow 从零开始学习理解及搭建一个Transformer，本文代码基于Tensorflow 2.0版本，若使用其他版本或Pytorch亦不妨参考。\n相较于 Tensorflow 官方指南，本文将对模块细节和使用进行更精细的讲解；将对代码构建过程中代码进行逐行解释以及自注意力机制等细节设计进行讲解。除此之外，也加入了笔者对于 Transformer 细节部分的一些个人疑惑和理解。简而言之，是融合了代码指南和原理精讲的一篇文章，力求将笔者对于 Transformer 的理解精粹于一篇文章，本人才疏学浅，欢迎批评指正。\n整个文章结构参考 Tensorflow 官方指南，按照自底向上的顺序来逐渐搭建一个用于将葡萄牙语翻译为英语的Transformer模型：先从最基本的算法模块实现，然后组装。在组装过程中讲解各个子模块产生的作用。\n2. 参考代码、文章及部分插图来源 本文大量参考及使用他人文章和官方文档中代码和图片，在此表示感谢：\nThe Illustrated Transformer Transformer model for language understanding TensorFlow Core r2.0 API Datasets v1.3.0 API 3. 在开始前的推荐了解 3.1. 循环神经网络（RNN） RNN是一种可用于抽取序列数据特征的神经网络结构，一个最基本的RNN是非常容易理解的，相当于 NLP 领域的 “Hello world“。很多 NLP 领域的知识点（如本文可能用到的注意力机制与自注意力机制、encoder-decoder架构）的相关文章很多都会使用一个最基本的 RNN 来类比），所以建议阅读本文前对最基本的循环神经网络有一个初步的了解。\n这部分内容推荐阅读 Ian Goodfellow 等人编写的 Deep Learning 一书中的 10.1 与 10.2 节。\n3.2. 基于编码-解码（encoder-decoder）的序列到序列（sequence2sequence）模型 在⾃然语⾔处理的很多应⽤中，输⼊和输出都可以是不定⻓序列。以机器翻译为例，输⼊可以是⼀段不定⻓的英语⽂本序列，输出可以是⼀段不定⻓的法语⽂本序列，当输⼊和输出都是不定⻓序列时，我们可以使⽤编码器—解码器（encoder-decoder）架构或者sequence2sequence模型。\n这部分内容推荐阅读 Ian Goodfellow 等人编写的 Deep Learning 一书中的 10.4 节。\n3.3. 注意力机制 Transformer 的核心思想——自注意力机制，实际上是受启发于注意力机制。对比注意力机制和自注意力机制的异同，可以更加的深刻理解自注意力机制的作用机理。\n强烈推荐阅读 张俊林 - 深度学习中的注意力模型。\n对英语有自信的同学可以阅读 Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)，其中包含大量的动画实例。\n3.4. 词嵌入（Word Embedding） 推荐阅读此文章 张俊林 - 从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史 的前半部分对 Word Embedding 的介绍。\n4. 初探 Transformer Transformer 是基于自注意力机制的全新 encoder-decoder 模型。相较于传统循环神经网络搭建的同类模型，Transformer 具有多重优势：解决长期依赖的问题，可以并行化等等。\n现在我们已经知道了一个 encoder-decoder 可以完成多种自然语言处理任务。为了举例方便，假设要完成一个机器翻译任务：从法语翻译到英语。Transformer 做的事情就是这样的：\n作为一个 encoder-decoder 模型，Transformer 将会被分为编码器（左侧浅绿色）和解码器（右侧粉色）两部分：\n如何设计这两个部分呢？论文中的图示是这样的：\n这一张图掩盖了许多细节，大量的箭头也使人不明就里。暂时来看，我们可以得到以下信息：\n解码器和编码器的结构类似，都是多头注意力机制（图中 Multi-Head Attention 块，是对于自注意力机制的变化使用）和前馈神经网络（Feed Forward）的堆叠多层。 输入不仅仅是句子的词嵌入表示，还额外增加了称作位置编码（Positional Encoding）的额外信息。 其他姑且按下不表。\n5. 基础算法和模块 5.1. 位置编码（Positional encoding） 一个传统的，使用 RNN 构建的 Encoder-Decoder 模型对于输入句子中的单词是逐个读取的：读取完前一个单词，更新模型的状态，然后在此状态下再读取下一个单词。这种方式天然的包含了句子的位置前后关系。\n我们后面会发现 Transformer 对于输入却并非逐个读取，而是对整个句子的每个单词进行同时读取。这种方式就显然丢失了句子的前后位置关系。\n为了能够保留句子中单词和单词之间的位置关系，需要将位置也融合进入输入的句子中，需要对位置进行编码。\nTransformer 使用的位置编码算法如下定义：\n$$P E_{(p o s, 2 i)}=\\sin \\left(\\text {pos} / 10000^{2 i / d_{\\text {model}}}\\right)$$\n$$P E_{(p o s, 2 i+1)}=\\cos \\left(\\text {pos} / 10000^{2 i / d_{\\text {matel}}}\\right)$$\n由公式来看，对于一个句子，此编码算法对于偶数位置 2i 和奇数位置 2i + 1 分开进行编码。编码的结果是每个位置最终转化为 d_model 维度的向量。\n一个对50个位置编码至 512 维度位置向量的图示例子如下（其作用于的输入就是句子，句子长至 50 单词，且每个词嵌入的维度为 512）：\n这个位置向量会直接加在词嵌入上使用，从而使词嵌入在含义远近的表示能力之外增加了句子中的位置关系的表示能力：\n位置编码向量被加到嵌入（embedding）向量中。嵌入表示一个 d 维空间的标记，在 d 维空间中有着相似含义的标记会离彼此更近。但是，嵌入并没有对在一句话中的词的相对位置进行编码。因此，当加上位置编码后，词将基于它们含义的相似度以及它们在句子中的位置，在 d 维空间中离彼此更近。\n才疏学浅，没法一个直观的角度来解释为何这样设计编码以及为何这样编码可以起到这样的效果。请各位指教。\n考虑此函数需要有大量的切片赋值操作（tensorflow api 实现起来较为困难，需要调用assign来实现numpy的赋值运算符=），所以使用numpy的api来实现，最后使用tf.cast将其自动转换为tensor（此处为一个eager tensor）。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def get_angles(pos, i, d_model): angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model)) return pos * angle_rates def positional_encoding(position, d_model): angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model) # 将 sin 应用于数组中的偶数索引（indices）；2i angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2]) # 将 cos 应用于数组中的奇数索引；2i+1 angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2]) pos_encoding = angle_rads[np.newaxis, ...] # 在这里增加了一个维度。eg: (50, 512) -\u0026gt; (1, 50, 512) # 为什么要增加一个维度呢？因为输入到 Transformer 的输入通常是多个句子层叠成一个批次。 # 增加一个维度就可以利用广播机制一次加法将一个批次的每个句子添加上位置编码。 return tf.cast(pos_encoding, dtype=tf.float32) 5.2. 注意力机制 准确来说，是按比缩放的点积注意力机制（Scaled dot product attention）。\n顾名思义，Transformer 一种点积注意力机制，并且在普通的点积注意力机制上增加了“按比缩放”这个特性。\n根据输入的不同，这个模块既可以是自注意力，也可以是非自注意力。\n如果已经阅读了本文第三部分推荐的注意力机制相关的文章张俊林 - 深度学习中的注意力模型，应该已经理解了注意力机制的本质：\n对于三个输入：\nQ：请求 query K：主键 key V：数值 value 注意力机制本质就是使用 Q 和 K 来计算出“注意力权重“，然后利用注意力权重对Ｖ进行加权求和。\n按比缩放的点积注意力定义如下：\n$$ \\text {Attention }(Q, K, V)=\\operatorname{softmax}{k}\\left(\\frac{Q K^{T}}{\\sqrt{d{k}}}\\right) V $$\n使用计算图来表示如下：\n可以看出，注意力权重是通过 Q 和 K 直接进行点积MatMul，并按比缩放Scale（除以深度的平方根），然后进行 softmax 得到的。\n同样的，使用注意力权重乘上 V 便得到了一次输出。\n有两点需要讨论：\n为什么要按比缩放Scale：这是原论文中的一个推测——如果输入的Q，K维度过大，则会导致点积后的结果很大softmax函数有一个特点，当输入的 x 越大，其梯度会趋近于 0。这对于基于梯度下降法的优化非常不利。（这个是一个有根据的推测：假设q和k都是独立的随机变量，那么q 乘上 k 是均值的 0 方差为 $d_k$ 的。除以深度的平方根，可以让方差为 1） 在 softmax 前，为什么有一个可选的 Mask 过程： 这是由于在整个模型的运行过程中，可能根据设计，要忽略掉一些输入。关于Mask的生成，将在下一部分详细解释。关于Mask的使用，将在#TODO时提及。 5.2.1. 计算步骤细节 注意：这个例子和实际 Transformer 使用的实际输入不同（并非直接使用词嵌入来作为Query、Key等输入），仅为了能够用自然语言阐述而设计。精确的解释我们放在后文。\n假设 Query 序列是一个句子，长度为 seq_len_q。\n而 Key 序列也是一些一个句子，长度为 seq_len_k。\nValue 序列中的每个值都对应 Key 句子中的一个单词，所以 v 的长度seq_len_v 等于 seq_len_k 。\n根据上文描述的注意力机制的计算方法：依次取 Query 中的单词，来和 Key 中的 seq_len_v 个单词一一计算注意力权重，得到 seq_len_v 个注意力权重，使用这个权重对 seq_len_k （再次强调，长度必须相同，这部分描述对应着下面代码注释中的要求2.）个 Value 进行加权求和，得到一个输出。\n一个 Query 序列将产生 seq_len_q 个加权求和。\n让我们用矩阵点乘的模式来描述这个过程：\n假设每个单词的嵌入为depth维度，显然这个步骤是:\nQ 矩阵(seq_len_q, depth) 和 K 矩阵(seq_len_k, depth) 的转置的矩阵乘法。\n最终得到注意力权重 (seq_len_q, seq_len_k) 矩阵。这个矩阵乘上 Value 矩阵(seq_len_v, depth_v) 便是加权求和过程，最终得到了输出Output (seq_len_q, depth_v)。\ndepth_v 表示 Value 序列中每个值的维度，显然，这个维度不一定要和上文的depth一致\n5.2.2. 使用向量化来提升效率 每个 Query 序列对应着一个 Key 序列，但这 Query-Key 组合彼此之间是独立的。完全可以将 Query、Key、Value 堆叠成批，一次运算搞定。矩阵乘法或是转置是针对最后的两个维度，所以只需要保持前置维度匹配（对应，下方注释的要求1.），计算结果和上面完全等效。\n举个例子：若有 batch_size 批次，每批次 N 条的 Query，Key。 其计算完全可以组织成 (batch_size, N, seq_len_q, depth)点乘(batch_size, N, seq_len_q, depth)的转置，最终得到(batch_size, N, seq_len_q, seq_len_k) 形状的张量。\nmask 以乘以一个极大的负数-1e9，然后在加上注意力权重，最终达到使一些位置的 Value 失效的效果。只需要保证其可通过广播机制能够自动转换为 (..., seq_len_q, seq_len_k) 形状（代码注释中的要求3.)。\n这一要求将会影响下一节的遮挡（Masking）的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def scaled_dot_product_attention(q, k, v, mask): \u0026#34;\u0026#34;\u0026#34; 这部分是对于输入的张量维度进行描述 1. q, k, v 必须具有匹配的前置维度。 2. k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。 3. 虽然 mask 根据其类型（填充或前瞻）有不同的形状，但是 mask 必须能进行广播转换以便求和。 参数: q: 请求的形状 == (..., seq_len_q, depth) k: 主键的形状 == (..., seq_len_k, depth) v: 数值的形状 == (..., seq_len_v, depth_v) mask: Float 张量，其形状能转换成 (..., seq_len_q, seq_len_k)。默认为None。 返回值: 输出，注意力权重 \u0026#34;\u0026#34;\u0026#34; matmul_qk = tf.matmul(q, k, transpose_b=True) # 得到张量形状为 (..., seq_len_q, seq_len_k) # 缩放 matmul_qk dk = tf.cast(tf.shape(k)[-1], tf.float32) # dk 即 depth scaled_attention_logits = matmul_qk / tf.math.sqrt(dk) # 将 mask 加入到缩放的张量上。 if mask is not None: scaled_attention_logits += (mask * -1e9) # softmax 在最后一个轴（seq_len_k）上归一化，因此分数 # 相加等于1。 attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) # (..., seq_len_q, seq_len_k) output = tf.matmul(attention_weights, v) # (..., seq_len_q, depth_v) return output, attention_weights 5.2.3. 如何作为自注意力使用 有人会这样总结：自注意力就是将同一个句子同时作为 Query 和 Key 来使用。我认为还不够精确。\n事实上，输入自注意力的 Query、Key 和 Value 都来自原始嵌入的线性变换：创建三个会随着模型训练过程不断优化的矩阵$W^Q$，$W^K$ 和 $W^V$，原始词嵌入乘上三个矩阵从而得到真正的 Query、Key 和 Value（也可以理解为三个独立的全连接神经网络，输入节点数量为词嵌入的维度，输出节点数量分别为Query，Key 和 Value 的维度）。\n其过程如下：\n其余过程则完全和上述过程一致：\n而自注意力的注意效果也是逐层变得集中。使用图形化来对自注意力的效果进行初步理解。\n当只经过线性变换的注意力效果：\n注意力显然非常分散，有种抓不到要领的感觉。\n而经过了5层自注意力机制，单个单词的注意力开始集中于少数部分。\n5.3. 遮挡 Mask 5.3.1. 填充遮挡 如果一个输入句子由于长短不一不方便计算或是其他原因需要补充一些填充标记（pad tokens），显然在输出结果的时候应该把这些无意义的填充标记排除，因此需要一个函数产生此用途的 mask。\n1 2 3 4 5 6 7 def create_padding_mask(seq): # 对于一个序列，如果某位置其值为0，则认定为填充标记，在此位置标 1 seq = tf.cast(tf.math.equal(seq, 0), tf.float32) # 为了能够利用广播机制匹配注意力权重张量，需要增加必要的维度 return seq[:, tf.newaxis, tf.newaxis, :] # (batch_size, 1, 1, seq_len) 测试效果：\n1 2 3 4 5 6 7 8 9 10 11 12 x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]]) create_padding_mask(x) \u0026#34;\u0026#34;\u0026#34; \u0026lt;tf.Tensor: id=207703, shape=(3, 1, 1, 5), dtype=float32, numpy= array([[[[0., 0., 1., 1., 0.]]], [[[0., 0., 0., 1., 1.]]], [[[1., 1., 1., 0., 0.]]]], dtype=float32)\u0026gt; \u0026#34;\u0026#34;\u0026#34; 5.3.2. 前瞻遮挡（look-ahead mask） 前瞻遮挡通常用于需要只考虑序列中的前一部分的时候，这个遮挡将会用在 Transform 的解码器部分，其设计原理是预测一个单词只考虑此单词前的单词，而不考虑此单词后的部分。\n1 2 3 4 def create_look_ahead_mask(size): # tf.linalg.band_path(Tensor, -1, 0) 是取Tensor的左下半矩阵 mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0) return mask # (seq_len, seq_len) 最终效果得到：\n1 2 3 4 5 6 7 8 9 x = tf.random.uniform((1, 3)) temp = create_look_ahead_mask(x.shape[1]) temp \u0026#34;\u0026#34;\u0026#34; \u0026lt;tf.Tensor: id=207718, shape=(3, 3), dtype=float32, numpy= array([[0., 1., 1.], [0., 0., 1.], [0., 0., 0.]], dtype=float32)\u0026gt; \u0026#34;\u0026#34;\u0026#34; 5.4. 多头注意力（Multi-head attention） 前文说到，使用一组会随着模型训练过程不断优化的矩阵$W^Q$，$W^K$ 和 $W^V$，可以通过对词嵌入 X 或者前一层的输出 R 相乘而得到一套可以输入进注意力机制的 Query, Key 和 Value。\n多头注意力机制是将初始的词向量（第一层）或前一层的输入（第二层开始）通过线性变换转换为多组 Query, Key 和 Value，从而得到不同的输出 Z。最后将所有的输出拼合起来，通过可训练的线性变换 $W^O$ 融合为一个输出：\n从注意力角度看，由于矩阵是随机初始化的，随着训练的过程，最终不同的Query, Key可能得到不同的注意力结果。\n论文认为：\n这种方式拓展了模型专注于不同位置的能力。 模型最终的“注意力”实际上是来自不同“表示子空间”的注意力的综合。 做个比喻来说，这就好像是八个有不同阅读习惯的翻译家一同翻译同一个句子，他们每个人可能翻译时阅读顺序和关注点都有所不同，综合他们八个人的意见，最终得出来的翻译结果可能会更加准确。\n通过继承 tf.keras.layers.Layer 可以对多头注意力机制进行结构清晰的实现。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 class MultiHeadAttention(tf.keras.layers.Layer): def __init__(self, d_model, num_heads): \u0026#34;\u0026#34;\u0026#34; 参数: d_model 必须能被 num_heads 整除 d_model: 由于要映射 num_heads 组 Q,K,V. d_model 的值需要为 num_heads * depth num_heads: 代表注意力机制的头数 \u0026#34;\u0026#34;\u0026#34; super(MultiHeadAttention, self).__init__() self.num_heads = num_heads self.d_model = d_model assert d_model % self.num_heads == 0 self.depth = d_model // self.num_heads self.wq = tf.keras.layers.Dense(d_model) self.wk = tf.keras.layers.Dense(d_model) self.wv = tf.keras.layers.Dense(d_model) self.dense = tf.keras.layers.Dense(d_model) def split_heads(self, x, batch_size): \u0026#34;\u0026#34;\u0026#34;分拆最后一个维度到 (num_heads, depth). 转置结果使得形状为 (batch_size, num_heads, seq_len, depth) \u0026#34;\u0026#34;\u0026#34; x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) return tf.transpose(x, perm=[0, 2, 1, 3]) def call(self, v, k, q, mask): batch_size = tf.shape(q)[0] # 这里采取将 q,k,v 先线性变换到 (..., seq_len, d_model) # 再拆分成 num_heads 份 (..., nums_heads, seq_ken, d_model / nums_heads) # 这和直接将原始 q,k,v 线性变换成 nums_heads 组是等效的！这样写效率更高！ q = self.wq(q) # (batch_size, seq_len, d_model) k = self.wk(k) # (batch_size, seq_len, d_model) v = self.wv(v) # (batch_size, seq_len, d_model) q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth) k = self.split_heads(k, batch_size) # (batch_size, num_heads, seq_len_k, depth) v = self.split_heads(v, batch_size) # (batch_size, num_heads, seq_len_v, depth) # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth) # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k) scaled_attention, attention_weights = scaled_dot_product_attention( q, k, v, mask) scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # (batch_size, seq_len_q, num_heads, depth) concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) # (batch_size, seq_len_q, d_model) output = self.dense(concat_attention) # (batch_size, seq_len_q, d_model) return output, attention_weights 5.4.1. 代码分析 由于维度多，两处更改reshape 和转置操作transpose令人头秃。所以来详细讲解一下Tensor是如何在多头注意力机制力流动的。如果对模型实现手到擒来的的同学可以直接跳过：\n首先，我们确定要输送给注意力机制 scaled_dot_product_attention 的形状是 (batch_size, nums_heads, seq_len_q, depth)，通俗来讲，注意力机制函数将会并行处理 batch_size 批句子，每批句子 nums_heads 句。为了得到合适的输出，要通过以下几步：\n将输入映射至足够维度(batch_size, seq_len, input_depth) -\u0026gt; (batch_size, seq_len, d_model) 1 2 3 4 5 ... q = self.wq(q) # (batch_size, seq_len, d_model) k = self.wk(k) # (batch_size, seq_len, d_model) v = self.wv(v) # (batch_size, seq_len, d_model) ... 这个 d_model 的深度显然大于我们需要的 depth，这是为了下一步拆成 num_heads，将多次线性变换，转换为一次。\n将输入拆分为多头 1 2 3 4 5 6 7 8 9 10 11 12 def split_heads(self, x, batch_size): \u0026#34;\u0026#34;\u0026#34;分拆最后一个维度到 (num_heads, depth). 转置结果使得形状为 (batch_size, num_heads, seq_len, depth) \u0026#34;\u0026#34;\u0026#34; x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth)) return tf.transpose(x, perm=[0, 2, 1, 3]) ... # 这里记得要把 batch_size 传进去 q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len_q, depth) k = self.split_heads(k, batch_size) # (batch_size, num_heads, seq_len_k, depth) v = self.split_heads(v, batch_size) # (batch_size, num_heads, seq_len_v, depth) ... 分拆最后一个维度到 (num_heads, depth)\n向量角度而言： reshape 操作将张量中每一行 d_model 都拆成了 num_heads 个 depth 长度的行向量。即：(batch_size, seq_len, d_model) -\u0026gt; (batch_size, seq_len, nums_heads, depth)。\n从神经网络角度而言：由于对于单层全连接网络，输入层与隐层节点的任何一个子集结合，都是一个完整的单隐层全连接网络。也就是说，这种拆分完全可以看做将前一步input_depth 个节点到 d_model 个节点的全连接网络，拆分成了 nums_heads 个小的 input_depth 个节点到 depth 个节点的全连接网络。\n然后，我们处理的仍然是序列本身，因此，通过转置 transpose 将 seq_len 放回它原来的位置，让 nums_heads 成为一个前置维度： (batch_size, seq_len, nums_heads, depth) -\u0026gt; (batch_size, num_heads, seq_len, depth)\n自注意力机制的详细运算过程已经在前文说的很清楚了，接下来是对输出的处理。\n将多头输出通过全连接映射为一个输出 显然自注意力机制函数的输出形状将是(batch_size, num_heads, seq_len_q, depth)。\n为了能够方便地将多头结果拼合起来，首先我们将其转置到倒数第二个维度。\n然后，2.中怎么拆开的，就怎么拼回去。\n1 2 3 4 5 6 ... scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # (batch_size, seq_len_q, num_heads, depth) concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) # (batch_size, seq_len_q, d_model) ... 最后，通过全连接层将拼合好的输出融合起来：\n1 2 3 ... output = self.dense(concat_attention) # (batch_size, seq_len_q, d_model) ... 这里，经过全连接层融合后的最后一维仍然是 d_model。\n5.5. 点式前馈网络（Point wise feed forward network） 点式前馈网络由两层全联接层组成，两层之间有一个 ReLU 激活函数。\n1 2 3 4 5 def point_wise_feed_forward_network(d_model, dff): return tf.keras.Sequential([ tf.keras.layers.Dense(dff, activation=\u0026#39;relu\u0026#39;), # 第一层输出尺寸 (batch_size, seq_len, dff) tf.keras.layers.Dense(d_model) # 第二层输出尺寸 (batch_size, seq_len, d_model) ]) dff: 规定了点式前馈神经网络的内部第一层输出节点。\n在论文中，这个神经网络被称作位置式前馈神经网络（Position-wise Feed-Forward Networks In，不知道为什么Tensorflow的文档要改名），其定义如下：\n$$ \\mathrm{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2} $$\n可以看出其本质上是两次线性变换的串联（不考虑激活函数的情况下）。\n我们考虑一个(..., seq_len, depth) 的输入，第一次线性变换相当于针对每一行（换句话说，句子的每个位置），做了一个相同（但隐层参数不同）的全连接网络，输入节点数 depth 和 输出节点数 dff。得到 （..., seq_len, dff)。同理第二次线性变换类似，得到(..., seq_len, d_model)。\n事实上，它只是一个多层神经网络，但考虑它等同地处理句子的每个位置，起名如此也算合理。\n6. 编码器解码器 有了上面的诸多模块，终于可以召唤此图：\n左侧编码器，右侧解码器。\n并开始组装编码器和解码器：\nTransformer 模型与标准的具有注意力机制的序列到序列模型（sequence to sequence with attention model），遵循相同的一般模式。\n输入语句经过 N 个编码器层，为序列中的每个词/标记生成一个输出。\n解码器关注编码器的输出以及它自身的输入（自注意力）来预测下一个词。\n从这里一直到到 Transformer 的完成，我们始终忽略 mask 的实际细节。由于这一部分涉及到模型的训练优化所以我们放在下一篇文章展开来讲。在此我们只把它当做一个参数暴露给外部模块。并传给内部模块。\n6.1. 编码器层 对于编码器，一个编码器层是其核心的最小单位。\n每个编码器层包括以下子层：\n多头注意力（有填充遮挡） 点式前馈网络（Point wise feed forward networks）。 每个子层在其周围有一个残差连接（图中最左侧的上下两个黑箭头），然后进行层归一化。残差连接有助于避免深度网络中的梯度消失问题。\n每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model（最后一个）维度完成的。Transformer 中有 N 个编码器层。\n此外对于每个子层的输出，都使用了0.1概率的的dropout。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class EncoderLayer(tf.keras.layers.Layer): def __init__(self, d_model, num_heads, dff, rate=0.1): super(EncoderLayer, self).__init__() self.mha = MultiHeadAttention(d_model, num_heads) self.ffn = point_wise_feed_forward_network(d_model, dff) self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6) self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6) self.dropout1 = tf.keras.layers.Dropout(rate) self.dropout2 = tf.keras.layers.Dropout(rate) def call(self, x, training, mask): # 填充遮挡将在调用时传入 attn_output, _ = self.mha(x, x, x, mask) # (batch_size, input_seq_len, d_model) attn_output = self.dropout1(attn_output, training=training) out1 = self.layernorm1(x + attn_output) # (batch_size, input_seq_len, d_model) ffn_output = self.ffn(out1) # (batch_size, input_seq_len, d_model) ffn_output = self.dropout2(ffn_output, training=training) out2 = self.layernorm2(out1 + ffn_output) # (batch_size, input_seq_len, d_model) return out2 6.2. 解码器层 每个解码器层包括以下子层：\n遮挡的多头注意力（前瞻遮挡和填充遮挡） 多头注意力（用填充遮挡）。V（数值）和 K（主键）接收编码器输出作为输入。Q（请求）接收遮挡的多头注意力子层的输出。 点式前馈网络 每个子层在其周围有一个残差连接，然后进行层归一化。每个子层的输出是 LayerNorm(x + Sublayer(x))。归一化是在 d_model（最后一个）维度完成的。 Transformer 中共有 N 个解码器层。\n当 Q 接收到解码器的第一个注意力块的输出，并且 K 接收到编码器的输出时，注意力权重表示根据编码器的输出赋予解码器输入的重要性。换一种说法，解码器通过查看编码器输出和对其自身输出的自注意力，预测下一个词。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class DecoderLayer(tf.keras.layers.Layer): def __init__(self, d_model, num_heads, dff, rate=0.1): super(DecoderLayer, self).__init__() self.mha1 = MultiHeadAttention(d_model, num_heads) self.mha2 = MultiHeadAttention(d_model, num_heads) self.ffn = point_wise_feed_forward_network(d_model, dff) self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6) self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6) self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6) self.dropout1 = tf.keras.layers.Dropout(rate) self.dropout2 = tf.keras.layers.Dropout(rate) self.dropout3 = tf.keras.layers.Dropout(rate) def call(self, x, enc_output, training, look_ahead_mask, padding_mask): # enc_output.shape == (batch_size, input_seq_len, d_model) attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask) # (batch_size, target_seq_len, d_model) attn1 = self.dropout1(attn1, training=training) out1 = self.layernorm1(attn1 + x) attn2, attn_weights_block2 = self.mha2( enc_output, enc_output, out1, padding_mask) # (batch_size, target_seq_len, d_model) attn2 = self.dropout2(attn2, training=training) out2 = self.layernorm2(attn2 + out1) # (batch_size, target_seq_len, d_model) ffn_output = self.ffn(out2) # (batch_size, target_seq_len, d_model) ffn_output = self.dropout3(ffn_output, training=training) out3 = self.layernorm3(ffn_output + out2) # (batch_size, target_seq_len, d_model) return out3, attn_weights_block1, attn_weights_block2 显然，相较于编码器，除了用于自注意力的多头注意力层。解码器增加了一层注意力层用于实现编码器和解码器之间的注意力。其他与编码器完全一致。\n注意：两层注意力使用的遮挡类型略有不同！\n6.3. 编码器 由于封装了上文实现的所有模块。所以这个模块的参数显得有些多，我们只关注此层特有的参数：\nnum_layers：规定编码器使用多少个编码器层。 input_vocab_size: 原语料的词汇量 maximum_position_encoding：最大的位置编码，代表位置编码最长匹配的位置长度 其总体步骤包括三步：\n将原始句子单词编码更换为词嵌入 将词嵌入加上位置编码 将处理过后的输出输入规定好的多个编码器层 其中：在原始句子的词嵌入上需要乘上 $\\sqrt{d_model}$，这是原论文中规定的。文章中没有对这个变量的解释。\nIn the embedding layers, we multiply those weights by $\\sqrt{d_{model}}$\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class Encoder(tf.keras.layers.Layer): def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1): super(Encoder, self).__init__() self.d_model = d_model self.num_layers = num_layers self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model) self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model) self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)] self.dropout = tf.keras.layers.Dropout(rate) def call(self, x, training, mask): seq_len = tf.shape(x)[1] # 将嵌入和位置编码相加。 x = self.embedding(x) # (batch_size, input_seq_len, d_model) x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) x += self.pos_encoding[:, :seq_len, :] x = self.dropout(x, training=training) for i in range(self.num_layers): x = self.enc_layers[i](x, training, mask) return x # (batch_size, input_seq_len, d_model) 6.4. 解码器 解码器和编码器参数类似。\n将输出序列同样转换为同维度词嵌入 加上位置编码 和编码器的输出一同输入给多个解码器层 在调用时，enc_output 是来自编码器层的输出。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 class Decoder(tf.keras.layers.Layer): def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1): super(Decoder, self).__init__() self.d_model = d_model self.num_layers = num_layers self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model) self.pos_encoding = positional_encoding(maximum_position_encoding, d_model) self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)] self.dropout = tf.keras.layers.Dropout(rate) def call(self, x, enc_output, training, look_ahead_mask, padding_mask): seq_len = tf.shape(x)[1] attention_weights = {} x = self.embedding(x) # (batch_size, target_seq_len, d_model) x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) x += self.pos_encoding[:, :seq_len, :] x = self.dropout(x, training=training) for i in range(self.num_layers): x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask) attention_weights[\u0026#39;decoder_layer{}_block1\u0026#39;.format(i+1)] = block1 attention_weights[\u0026#39;decoder_layer{}_block2\u0026#39;.format(i+1)] = block2 # x.shape == (batch_size, target_seq_len, d_model) return x, attention_weights 7. 创建 Transformer 将编码器和解码器组合起来，连接最后的线性输出层，就得到了整体的 Transformer：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 class Transformer(tf.keras.Model): def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1): super(Transformer, self).__init__() self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate) self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate) self.final_layer = tf.keras.layers.Dense(target_vocab_size) def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask): enc_output = self.encoder(inp, training, enc_padding_mask) # (batch_size, inp_seq_len, d_model) # dec_output.shape == (batch_size, tar_seq_len, d_model) dec_output, attention_weights = self.decoder( tar, enc_output, training, look_ahead_mask, dec_padding_mask) final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size) return final_output, attention_weights 8. 小结 到此为止，我们已经描述了 Transformer 的整个模型搭建过程，并逐层逐行地解释了其正向传播的原理和细节。本文仍未讲到的是：\n如何训练一个Transformer： 前瞻遮挡和填充遮挡如何使用。 超参数如何配置。 如何设计损失函数。 如何优化和评估。 后篇将结合实例详细描述这些内容。\n","date":"2019-12-02T00:00:00Z","image":"https://lilithsangreal.com/p/transformer-from-zero-1/the_transformer_hu9dc15172e42e840ef18a078a4efb196f_44824_120x120_fill_box_smart1_3.png","permalink":"https://lilithsangreal.com/p/transformer-from-zero-1/","title":"从 0 开始学习 Transformer 上篇：Transformer 搭建与理解"}]